{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Preprocessing for NLP\n",
        "\n",
        "## Dataset:\n",
        "**Twitter Sentiment Analysis (Sentiment140)**\n",
        "- https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "\n",
        "This dataset contains 1.6 million tweets with sentiment labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once if needed)\n",
        "# !pip install nltk spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/prince/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/prince/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/prince/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/prince/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (1600000, 6)\n"
          ]
        }
      ],
      "source": [
        "# Load the Kaggle Sentiment140 dataset\n",
        "data_path = \"training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "# The dataset has no header, so we specify column names\n",
        "columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "\n",
        "df = pd.read_csv(data_path, encoding='latin-1', names=columns)\n",
        "print(f\"Dataset shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target         ids                          date      flag  \\\n",
              "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
              "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
              "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
              "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
              "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
              "\n",
              "              user                                               text  \n",
              "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
              "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
              "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
              "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
              "4           Karoli  @nationwideclass no, it's not behaving at all....  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample size: (1000, 6)\n"
          ]
        }
      ],
      "source": [
        "# Use a smaller sample for faster processing\n",
        "df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
        "print(f\"Sample size: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@misstoriblack cool , i have no tweet apps&nbsp;&nbsp;fo...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@TiannaChaos i know&nbsp;&nbsp;just family drama. its la...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>School email won't open&nbsp;&nbsp;and I have geography ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>upper airways problem</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Going to miss Pastor's sermon on Faith...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>on lunch....dj should come eat with me</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@piginthepoke oh why are you feeling like that?</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gahh noo!peyton needs to live!this is horrible</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@mrstessyman thank you glad you like it! There...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text sentiment\n",
              "0             @chrishasboobs AHHH I HOPE YOUR OK!!!   negative\n",
              "1  @misstoriblack cool , i have no tweet apps  fo...  negative\n",
              "2  @TiannaChaos i know  just family drama. its la...  negative\n",
              "3  School email won't open  and I have geography ...  negative\n",
              "4                             upper airways problem   negative\n",
              "5         Going to miss Pastor's sermon on Faith...   negative\n",
              "6            on lunch....dj should come eat with me   positive\n",
              "7   @piginthepoke oh why are you feeling like that?   negative\n",
              "8    gahh noo!peyton needs to live!this is horrible   negative\n",
              "9  @mrstessyman thank you glad you like it! There...  positive"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert target: 0 -> negative, 4 -> positive\n",
        "df['sentiment'] = df['target'].map({0: 'negative', 4: 'positive'})\n",
        "df[['text', 'sentiment']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. Lowercase Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
            "After: @chrishasboobs ahhh i hope your ok!!! \n"
          ]
        }
      ],
      "source": [
        "print(\"Before:\", df['text'][0])\n",
        "df['text_clean'] = df['text'].str.lower()\n",
        "print(\"After:\", df['text_clean'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: Check this https://example.com for more\n",
            "After: Check this  for more\n"
          ]
        }
      ],
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "# Test\n",
        "test_text = \"Check this https://example.com for more\"\n",
        "print(\"Before:\", test_text)\n",
        "print(\"After:\", remove_urls(test_text))\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(remove_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. Remove HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: This is <b>bold</b> and <br/> new line\n",
            "After: This is bold and  new line\n"
          ]
        }
      ],
      "source": [
        "def remove_html_tags(text):\n",
        "    html_pattern = re.compile(r'<.*?>')\n",
        "    return html_pattern.sub('', text)\n",
        "\n",
        "# Test\n",
        "test_text = \"This is <b>bold</b> and <br/> new line\"\n",
        "print(\"Before:\", test_text)\n",
        "print(\"After:\", remove_html_tags(test_text))\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(remove_html_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4. Remove Twitter Elements (@mentions, #hashtags, RT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: RT @john: I love #MachineLearning @everyone\n",
            "After: RT : I love MachineLearning \n"
          ]
        }
      ],
      "source": [
        "def remove_twitter_elements(text):\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove @mentions\n",
        "    text = re.sub(r'#', '', text)      # Remove # but keep word\n",
        "    text = re.sub(r'\\brt\\b', '', text) # Remove RT\n",
        "    return text\n",
        "\n",
        "# Test\n",
        "test_text = \"RT @john: I love #MachineLearning @everyone\"\n",
        "print(\"Before:\", test_text)\n",
        "print(\"After:\", remove_twitter_elements(test_text))\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(remove_twitter_elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5. Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: Hello! How are you? I'm great...\n",
            "After: Hello How are you Im great\n"
          ]
        }
      ],
      "source": [
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Test\n",
        "test_text = \"Hello! How are you? I'm great...\"\n",
        "print(\"Before:\", test_text)\n",
        "print(\"After:\", remove_punctuation(test_text))\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. Chat Word Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: u r gr8 lol thx btw\n",
            "After: you are great laugh out loud thanks by the way\n"
          ]
        }
      ],
      "source": [
        "chat_words = {\n",
        "    'u': 'you', 'ur': 'your', 'r': 'are', 'y': 'why',\n",
        "    'pls': 'please', 'plz': 'please', 'thx': 'thanks',\n",
        "    'ty': 'thank you', 'bc': 'because', 'b4': 'before',\n",
        "    'gr8': 'great', 'l8r': 'later', 'w8': 'wait',\n",
        "    'omg': 'oh my god', 'lol': 'laugh out loud',\n",
        "    'brb': 'be right back', 'btw': 'by the way',\n",
        "    'idk': 'i do not know', 'tbh': 'to be honest',\n",
        "    'gonna': 'going to', 'wanna': 'want to',\n",
        "    'dont': 'do not', 'cant': 'cannot', 'wont': 'will not'\n",
        "}\n",
        "\n",
        "def convert_chat_words(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([chat_words.get(w, w) for w in words])\n",
        "\n",
        "# Test\n",
        "test_text = \"u r gr8 lol thx btw\"\n",
        "print(\"Before:\", test_text)\n",
        "print(\"After:\", convert_chat_words(test_text))\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(convert_chat_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7. Remove Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub('', text)\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(remove_emojis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 8. Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of stopwords: 198\n",
            "Before: this is a sample sentence with stopwords\n",
            "After: sample sentence stopwords\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(f\"Number of stopwords: {len(stop_words)}\")\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([w for w in words if w not in stop_words])\n",
        "\n",
        "# Test\n",
        "test_text = \"this is a sample sentence with stopwords\"\n",
        "print(\"Before:\", test_text)\n",
        "print(\"After:\", remove_stopwords(test_text))\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 9. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split(): ['Hello', 'world', 'how', 'are', 'you']\n",
            "regex: ['Hello', 'world', 'how', 'are', 'you']\n",
            "NLTK: ['Hello', 'world', 'how', 'are', 'you']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello world how are you\"\n",
        "\n",
        "# Method 1: split()\n",
        "print(\"split():\", text.split())\n",
        "\n",
        "# Method 2: Regex\n",
        "print(\"regex:\", re.findall(r'\\b\\w+\\b', text))\n",
        "\n",
        "# Method 3: NLTK\n",
        "print(\"NLTK:\", word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Method 4: spaCy (best for production)\n",
        "# Run these two lines first (only once) to install spaCy:\n",
        "!pip install spacy -q\n",
        "!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy: ['Hello', 'world', 'how', 'are', 'you']\n"
          ]
        }
      ],
      "source": [
        "# Now use spaCy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "print(\"spaCy:\", [token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 10. Stemming\n",
        "Reduces words to root form (fast, but may produce non-words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running -> run\n",
            "runs -> run\n",
            "easily -> easili\n",
            "happily -> happili\n"
          ]
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['running', 'runs', 'easily', 'happily']\n",
        "for word in words:\n",
        "    print(f\"{word} -> {stemmer.stem(word)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 11. Lemmatization\n",
        "Reduces words to dictionary form (slower, but more accurate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running -> running\n",
            "runs -> run\n",
            "easily -> easily\n",
            "cats -> cat\n",
            "better -> better\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'runs', 'easily', 'cats', 'better']\n",
        "for word in words:\n",
        "    print(f\"{word} -> {lemmatizer.lemmatize(word)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_lemmatization(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(w) for w in words])\n",
        "\n",
        "df['text_clean'] = df['text_clean'].apply(apply_lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Tweet 1 ---\n",
            "Original: @chrishasboobs AHHH I HOPE YOUR OK!!! \n",
            "Cleaned:  ahhh hope ok\n",
            "Sentiment: negative\n",
            "\n",
            "--- Tweet 2 ---\n",
            "Original: @misstoriblack cool , i have no tweet apps  for my razr 2\n",
            "Cleaned:  cool tweet apps razr 2\n",
            "Sentiment: negative\n",
            "\n",
            "--- Tweet 3 ---\n",
            "Original: @TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u\n",
            "Cleaned:  know family drama lamehey next time hang kim n guy like sleepover whatever ill call\n",
            "Sentiment: negative\n",
            "\n",
            "--- Tweet 4 ---\n",
            "Original: School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(\n",
            "Cleaned:  school email open geography stuff revise stupid school\n",
            "Sentiment: negative\n",
            "\n",
            "--- Tweet 5 ---\n",
            "Original: upper airways problem \n",
            "Cleaned:  upper airway problem\n",
            "Sentiment: negative\n"
          ]
        }
      ],
      "source": [
        "# Compare original vs cleaned\n",
        "for i in range(5):\n",
        "    print(f\"\\n--- Tweet {i+1} ---\")\n",
        "    print(f\"Original: {df['text'][i]}\")\n",
        "    print(f\"Cleaned:  {df['text_clean'][i]}\")\n",
        "    print(f\"Sentiment: {df['sentiment'][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
              "      <td>ahhh hope ok</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@misstoriblack cool , i have no tweet apps&nbsp;&nbsp;fo...</td>\n",
              "      <td>cool tweet apps razr 2</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@TiannaChaos i know&nbsp;&nbsp;just family drama. its la...</td>\n",
              "      <td>know family drama lamehey next time hang kim n...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>School email won't open&nbsp;&nbsp;and I have geography ...</td>\n",
              "      <td>school email open geography stuff revise stupi...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>upper airways problem</td>\n",
              "      <td>upper airway problem</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Going to miss Pastor's sermon on Faith...</td>\n",
              "      <td>going miss pastor sermon faith</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>on lunch....dj should come eat with me</td>\n",
              "      <td>lunchdj come eat</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@piginthepoke oh why are you feeling like that?</td>\n",
              "      <td>oh feeling like</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gahh noo!peyton needs to live!this is horrible</td>\n",
              "      <td>gahh noopeyton need livethis horrible</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@mrstessyman thank you glad you like it! There...</td>\n",
              "      <td>thank glad like product review bit site enjoy ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
              "1  @misstoriblack cool , i have no tweet apps  fo...   \n",
              "2  @TiannaChaos i know  just family drama. its la...   \n",
              "3  School email won't open  and I have geography ...   \n",
              "4                             upper airways problem    \n",
              "5         Going to miss Pastor's sermon on Faith...    \n",
              "6            on lunch....dj should come eat with me    \n",
              "7   @piginthepoke oh why are you feeling like that?    \n",
              "8    gahh noo!peyton needs to live!this is horrible    \n",
              "9  @mrstessyman thank you glad you like it! There...   \n",
              "\n",
              "                                          text_clean sentiment  \n",
              "0                                       ahhh hope ok  negative  \n",
              "1                             cool tweet apps razr 2  negative  \n",
              "2  know family drama lamehey next time hang kim n...  negative  \n",
              "3  school email open geography stuff revise stupi...  negative  \n",
              "4                               upper airway problem  negative  \n",
              "5                     going miss pastor sermon faith  negative  \n",
              "6                                   lunchdj come eat  positive  \n",
              "7                                    oh feeling like  negative  \n",
              "8              gahh noopeyton need livethis horrible  negative  \n",
              "9  thank glad like product review bit site enjoy ...  positive  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[['text', 'text_clean', 'sentiment']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Complete Pipeline Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: @john OMG!! Check https://test.com #AI is gr8!!!\n",
            "Cleaned: oh my god check ai great\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = text.split()\n",
        "    words = [chat_words.get(w, w) for w in words]\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Test\n",
        "test = \"@john OMG!! Check https://test.com #AI is gr8!!!\"\n",
        "print(\"Original:\", test)\n",
        "print(\"Cleaned:\", preprocess_text(test))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
